{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD4Qzglp3J-h"
      },
      "source": [
        "#Download the data \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cCyU-vV5Yb0"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/PranjalGupta-1/CHATBOT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiUyHP4T2g5F"
      },
      "source": [
        "# Install the dependicies\n",
        "Run the code below to install the dependicies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LL4rxT6_W7h"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index==0.5.6\n",
        "!pip install langchain==0.0.148\n",
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbuYetOy25eM"
      },
      "source": [
        "# Define the functions\n",
        "The following code defines the functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UelAqQgk_yIt"
      },
      "outputs": [],
      "source": [
        "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def construct_index(directory_path):\n",
        "    # set maximum input size\n",
        "    max_input_size = 4096\n",
        "    # set number of output tokens\n",
        "    num_outputs = 2000\n",
        "    # set maximum chunk overlap\n",
        "    max_chunk_overlap = 20\n",
        "    # set chunk size limit\n",
        "    chunk_size_limit = 600 \n",
        "\n",
        "    # define prompt helper\n",
        "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
        "\n",
        "    # define LLM\n",
        "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n",
        " \n",
        "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
        "    \n",
        "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    index.save_to_disk('index.json')\n",
        "\n",
        "    return index\n",
        "def load_documents_from_directory(directory_path):\n",
        "    documents = []\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith('.txt') or filename.endswith('.csv') or filename.endswith('.pdf'):\n",
        "            filepath = os.path.join(directory_path, filename)\n",
        "            if filename.endswith('.pdf'):\n",
        "                content = extract_text_from_pdf(filepath)\n",
        "            else:\n",
        "                with open(filepath, 'r') as file:\n",
        "                    content = file.read()\n",
        "            documents.append(content)\n",
        "    return documents\n",
        "\n",
        "def extract_text_from_pdf(filepath):\n",
        "    with open(filepath, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def Pr_BOT():\n",
        "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
        "    while True: \n",
        "        query = input(\"Welcone! to the Pr-BoT \")\n",
        "        response = index.query(query)\n",
        "        display(Markdown(f\"Response: <b>{response.response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz1jp33jGumu"
      },
      "source": [
        "# Set OpenAI API Key\n",
        "OPENAI API key \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = input(\"Paste your OpenAI key here\")"
      ],
      "metadata": {
        "id": "nBBgAYFIN4fa",
        "outputId": "e159ebf4-5af4-4a19-9aec-cefd2375ca11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your OpenAI key here and hit enter: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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVrddlAL4I_v"
      },
      "source": [
        "#**Construct** an index\n",
        "Construct the index. This will take every file in the folder 'data', split it into chunks, and embed it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "construct_index(\"CHATBOT\")"
      ],
      "metadata": {
        "id": "GYn7B2wyN1-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a513059-d5e5-400a-9879-dbbf45d5cd81"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.indices.vector_store.vector_indices.GPTSimpleVectorIndex at 0x7f404050e680>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipJ_gYxN5cWh"
      },
      "source": [
        "#Ask questions\n",
        " Run the function that queries GPT and type your question into the input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_uwsPGEIGsb"
      },
      "outputs": [],
      "source": [
        "Pr_BOT()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}